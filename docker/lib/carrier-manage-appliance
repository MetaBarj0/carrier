#!/bin/sh

# source and validate (or invalidate) the appliance's manifest file. Ensure all
# mandatory variables are set. This is not a deep validation, this functions
# only verify the exposition of a global variable.
appliance_shallowValidateManifestFile() {
  # first, source the manifest file
  . "$MANIFEST_FILE_PATH"

  # then, testing mandatory variables existence
  if [ -z "$COMPOSE_BUILD_CONTEXT" ]; then
    error "$(
    cat << EOI
The appliance's manifest file is ill formed as it lacks one mandatory variable
values.
In order to build an appliance, you must provide a value for the following
mandatory variable :

  COMPOSE_BUILD_CONTEXT

This variable maps a service's image name (in your docker-compose.yml file)
and a function name (defined in the manifest file). With these elements, the
appliance management system will be able to generate a Dockerfile for each
service of your appliance and build this image.

Taking into account this remark should help you to resolve your
problem...exiting...
EOI
    )"
    return 1
  fi

  # shallow validation complete, the appliance name is gotten, assuming all is
  # OK so far
  APPLIANCE_NAME="$(basename "$(dirname "$MANIFEST_FILE_PATH")")"
}

# help message on how to use the appliance management system.
appliance_showUsage() {
  error "$(
  cat << EOI
Invalid arguments specified. Usage :

  manage-appliance <command> <appliance> where

<command> is either :

  build
  start
  stop

and <appliance> is either :
- an absolute path to a manifest file of an appliance
- a relative path of a manifest file of an appliance
- an appliance name

Example :

  manage-appliance start devenv_cpp

will attempt to start the devenv_cpp appliance.

Notes :
- If an appliance is asked for starting and is not built beforehand, the system
  will attempt to build it first automatically.
- Attempting to stop an appliance that is not started does nothing.
EOI
  )"
}

# check the argument provided by the user to this script. Ensure that arguments
# are correct before going any further
appliance_checkArgument() {
  # TODO : the command should be known, no necessary to cheack here, has been
  # checked before
  COMMAND="$1"

  # verify the command specified, if invalid, returns 1
  if [ "$COMMAND" != 'build' ] \
     && [ "$COMMAND" != 'start' ] \
     && [ "$COMMAND" != 'stop' ]; then
    appliance_showUsage
    return 1
  fi

  # try to resolve the manifest file path by:
  #  - appending it to the current user directory, useful if relative path is
  #    provided
  #  - testing directly the file path, useful if absolute path is provided
  # TODO : refactor that, same mechanics ofr image building
  local manifest_arg="$2"
  if [ ! -f "${USER_DIRECTORY}"/"$manifest_arg" ] \
     && [ ! -f "$manifest_arg" ]; then
    # not a file path, testing the appliance name
    manifest_arg="${DOCKER_APPLIANCES_DIRECTORY}"/"$2"/manifest
    if [ ! -f "${manifest_arg}" ]; then
      appliance_showUsage
      return 1
    fi
  fi

  # OK, valid argument, exposing the manifest file absolute path
  MANIFEST_FILE_PATH="${manifest_arg}"
}

# put all necessary files intor the being built appliance staging directory.
appliance_fillStagingArea() {
  # TODO : take a look at filling staging area in the image build feature to
  #        see if a refacto could be done
  local appliance_directory="$(dirname "$MANIFEST_FILE_PATH")"

  # create the intermediate appliance directory in the staging area if it does
  # not already exist
  local staging_directory=${CARRIER_TMP_DIR}/appliances
  APPLIANCE_STAGING_DIRECTORY="${staging_directory}"/"$APPLIANCE_NAME"

  # sanitize the staging area if it does already exist
  rm -rf "$APPLIANCE_STAGING_DIRECTORY"
  mkdir -p "$staging_directory"
  cp -a "$appliance_directory" "$staging_directory"


  # copy necessary lib files in the project directory in the staging area
  cd "$CARRIER_LIB_DIR"

  cp \
    .dockerignore \
    exportPackageTo \
    functions.sh \
    importPackageFrom \
    "$APPLIANCE_STAGING_DIRECTORY"

  cd "$USER_DIRECTORY"
}

# fetch a copy of the master branch of the carrier directory then, copy the
# entire docker directory into the staging directory. This manifest is used to
# build various dependencies and serve as a genuine repository.
appliance_fetchManifest() {
  # use the metabarj0/manifest docker image that MUST exist
  local manifest_image="$(
    docker image ls --format='{{.Repository}}' metabarj0/manifest)"

  if [ -z "$manifest_image" ]; then
    error "$(cat << EOI
Error: cannot find the metabarj0/manifest docker image on your docker host. This
image is built by the bootstrap process.
Possible causes are :

  - you did not bootstrap your system. See bootstrap documentation.
  - you accidentally (or not) removed or untagged the metabarj0/bootstrap docker
    image.

Taking these remarks into account should help you to resolve your
problem...exiting...
EOI
    )"
    return 1
  fi

  # call to a functions.sh's utility
  fetchManifestImageContent "$APPLIANCE_STAGING_DIRECTORY"
}

# build an image that is required for the appliance and that was not found on
# the docker host. Relies on the image build system.
appliance_buildDependentImage() {
  # uses the metabarj0/manifest container to build required images. It'll result
  # in the creation of a docker directory inside the appliance staging directory
  appliance_fetchManifest

  local required_image_name="$1"

  # look for the manifest file providing this image, in genuine location, the
  # fetched manifest
  # TODO : same remark in build image system : source the variable value
  #        instead of sed-ing it
  local image_manifest_path="$(
    find "$FETCHED_MANIFEST" \
      -type f \
      -name manifest \
      -exec grep -EH 'PROVIDES='"$required_image_name"'$' {} \; \
      | sed -E 's/(.+):.*$/\1/')"

  if [ -z "$image_manifest_path" ]; then
    error "$( cat << EOI
Error: cannot find a suitable project to build the "$required_image_name".
Possible causes are :

  - mis-spelled required image name
  - project does not exist in the docker/share/images directory
  - project exists in your local copy of the docker/share/images directory but
    not on the distant repository that is fetched to build your dependent image
  - project is an image provided by the bootstrap process and
    your system is not bootstrapped. Such images are metabarj0/gcc,
    metabarj0/make... See bootstrap documentation for more details
  - The project exists on the distant repository and has all the necessary to be
    built BUT the 'PROVIDES' variable in the manifest file is not defined on an
    unique line (use the '\\' character after '=' for instance).

Taking these remarks into account should help you to solve you
problem...exiting
EOI
    )"

    return 1
  fi

  # I've the absolute path of the required image's manifest file, let's build it
  # in a new sub-shell
  # TODO : using carrier directly here; check if it breaks something
  ( ${CARRIER_BIN_DIR}/carrier image build "$image_manifest_path" )

  # TODO : check if everything was good before continuing, image build may fail
}

# enumerates all required images needed by the appliance and build them if
# they are not on the docker host
appliance_checkRequiredImages() {
  # manifest file read and validated, check existing images and attempt build
  # them if necessary
  local image=
  for image in $REQUIRED_IMAGES; do
    local image_name="$(
      docker image ls --format='{{.Repository}}' $image)"

    # image does not exist on the docker host
    if [ -z "$image_name" ]; then
      appliance_buildDependentImage "$image"
    fi
  done
}

# Generate the entire Dockerfile of an image of the appliance, relying on the
# content of the manifest file and values computed by the process of appliance
# building.
appliance_compileDockerfile() {
  # TODO refactor this function by breaking it in more atomic parts
  # create a map between image used in build stages and their alias that will be
  # used later
  local names_aliases_map="$(
    mapImageNamesAndBuildStageAliases "$REQUIRED_IMAGES"
    )"

  # create a Dockerfile in the appliance staging directory. Using required image
  # specified in the manifest as well as other variables :
  local dependencies_build_stages=
  local build_stage=
  local pair=
  local name=
  local alias=
  for pair in $names_aliases_map; do
    # extract image name and build stage alias
    name="$(keyOf "$pair")"
    alias="$(valueOf "$pair")"

    # instruction to export the dependency image package
    build_stage="$(cat << EOI
FROM $name as $alias
USER root:root
RUN exportPackageTo /tmp/package
EOI
    )"

    # group these instructions together
    dependencies_build_stages="$(
      append "$dependencies_build_stages" \
             "$build_stage" \
             $'\n'
    )"
  done

  # copy tooling to extract dependencies' packages
  local appliance_build_stage="$(cat << EOI
FROM $BASE_IMAGE as final
RUN mkdir -p /usr/local/bin/
COPY importPackageFrom /usr/local/bin
EOI
  )"

  # import each dependency image package
  local appliance_image_imports=
  local dependency_import=
  for pair in $names_aliases_map; do
    # extract build stage alias
    alias="$(valueOf "$pair")"

    # create import instruction for each dependency
    dependency_import="$(cat << EOI
COPY --from=$alias /tmp/package /tmp/
RUN importPackageFrom /tmp/package
EOI
    )"

    # aggregate them
    appliance_image_imports="$(
      append "$appliance_image_imports" \
             "$dependency_import" \
             $'\n'
    )"
  done

  # final step, create the Dockerfile and appending the
  # EXTRA_DOCKERFILE_COMMANDS
  cat << EOI > "${APPLIANCE_STAGING_DIRECTORY}"/Dockerfile
$dependencies_build_stages
$appliance_build_stage
$appliance_image_imports
RUN rm /usr/local/bin/importPackageFrom
$EXTRA_DOCKERFILE_COMMANDS
EOI
}

# create the service's docker image on the docker host using the generated
# Dockerfile
appliance_buildServiceImage() {
  # now we have a dockerfile, we just have to use it to build the appliance
  # image, go to the build context
  cd "$APPLIANCE_STAGING_DIRECTORY"

  # build a squashed image, docker compose can't do that yet
  docker build --squash -t "$IMAGE_NAME" .

  # TODO : log log log !!!

  # cleanup intermediate images, docker compose don't do that yet
  docker image prune -f

  # TODO : log log log !!!

  cd "$USER_DIRECTORY"
}

# cleanup the appliance staging area when the build is complete
appliance_cleanupStagingArea() {
  if [ -d "$APPLIANCE_STAGING_DIRECTORY" ]; then
    rm -rf "$APPLIANCE_STAGING_DIRECTORY"
  fi

  if [ -z "$(ls "${DOCKER_TMP_DIRECTORY}"/appliances)" ]; then
    rm -r "${DOCKER_TMP_DIRECTORY}"/appliances
  fi
}

# source the specified function, exposing glabal variables useful to build a
# service image.
# $1 : function name
appliance_callBuildContextFunction() {
  # execute the function
  "$1"

  # verify the presence of a mandatory variable, that must be defined by the
  # function execution
  if [ -z "$BASE_IMAGE" ]; then
    error "$(cat << EOI
Error: the BASE_IMAGE variable is not set.
This variable is mandatory to build your service image as it is the name of the
base image.

This variable must be exposed in the body of your build context function inside
the manifest file of your appliance.

Note that this variable must be global as a local variable has no definition
outside its function scope.

Taking this remark into account should help you to resolve your
problem...exiting...
EOI
    )"
    return 1
  fi
}

# unset global variables exposed by one of the manifest's function call
appliance_unsetBuildContextVariables() {
  unset IMAGE_NAME
  unset BASE_IMAGE
  unset REQUIRED_IMAGES
  unset EXTRA_DOCKERFILE_COMMANDS
}

# this function will build each and every image that are needed to build each
# service of the appliance. It relies on the content of the manifest file that
# has been source and roughly validated
appliance_buildServiceImages() {
  # a common part of an error message
  local part_error_message="$(cat << EOI
Make sure that the COMPOSE_BUILD_CONTEXT variable is defined like following :

  COMPOSE_BUILD_CONTEXT="image_name1 function_name1
image_name2 function_name2
image_name3 function_name3" and so on...

Taking this remark into account should help you to resolve your
problem...exiting
EOI
)"

  # iterate through each pair of image name/function to build
  local pair_element=
  local function_name=
  for pair_element in $COMPOSE_BUILD_CONTEXT; do
    # extract pair's elements
    if [ -z "$IMAGE_NAME" ]; then
      # first, verify current pair_element
      if [ -z "$pair_element" ]; then
        error "$(cat << EOI
Error: Cannot read the image name in the definition of COMPOSE_BUILD_CONTEXT in
your appliance manifest file.
$part_error_message
EOI
        )"
        return 1
      fi

      # extracting the image name
      IMAGE_NAME="$pair_element"

      # loop at once to get the function name
      continue
    else
      # image name is set, assuming we are reading a function name but first,
      # make a check
      if [ -z "$pair_element" ]; then
        error "$(cat << EOI
Error: Cannot read the function name in the definition of COMPOSE_BUILD_CONTEXT
in your appliance manifest file.
$part_error_message
EOI
        )"
        return 1
      fi

      # extracting the function name
      function_name="$pair_element"
    fi

    # image already exists, nothing to do
    local repository_id="$(docker image ls -q "$IMAGE_NAME")"
    if [ ! -z "$repository_id" ]; then
      # some global variable have been set, rollback context
      IMAGE_NAME=
      function_name=
      continue
    fi

    # validate and run the function, exposing global variables to build the
    # image
    appliance_callBuildContextFunction "$function_name"

    # check if requested docker images are built. If not, the system will attempt
    # to build them.
    appliance_checkRequiredImages

    # then, generate the dockerfile to build the appliance docker image
    appliance_compileDockerfile

    # then, build the image
    appliance_buildServiceImage

    # unset all global variables set by the build context function call
    appliance_unsetBuildContextVariables

    # resetting the function name, for one more iteration
    function_name=
  done
}

# entrypoint of the build command. In finality, create the appliance's docker
# image on the docker host.
appliance_build() {
  # prepare the staging area to build the appliance
  appliance_fillStagingArea

  # build the image of each service
  appliance_buildServiceImages

  # fnally, make some cleanup in the staging area
  appliance_cleanupStagingArea
}

# entrypoint of the start command. If the appliance requested to start has not
# been built yet, the system will attempt to build it first.
appliance_start() {
  # if some image are not yet built, build them
  appliance_build

  cd "$APPLIANCE_STAGING_DIRECTORY"

  # crafting the docker-compose command, inhibiting image creation that is
  # handled by this script and forcing container recreation, containers being
  # ephemeral
  local command='docker-compose up --no-build -d'

  # execute the docker-compose command
  eval "$command"

  cd "$USER_DIRECTORY"

  appliance_cleanupStagingArea
}

# entrypoint for the stop command. If the appliance is running, stops it.
appliance_stop() {
  appliance_fillStagingArea

  cd "$APPLIANCE_STAGING_DIRECTORY"

  # crafting the docker-compose command, removing ephemeral containers
  local command='docker-compose rm -sf'

  # execute the docker-compose command
  eval "$command"

  cd "$USER_DIRECTORY"

  appliance_cleanupStagingArea

  info 'Appliance '"$APPLIANCE_NAME"' successfully stopped.'
}

# generic function executing the right function depending on the command
# provided. The name of the function to call is built appending the command name
# and 'Appliance'
appliance_executeRequestedCommand() {
  # build the function name according to the provided command
  local function_name=appliance_${COMMAND}

  # make sure the manifest file is well formed and contains mandatory stuff
  appliance_shallowValidateManifestFile

  # simply call the function
  $function_name
}

### main entrypoint to manage appliances (build, start, stop, â€¦)
appliance_manage() {
  appliance_checkArgument "$1" "$2"
  appliance_executeRequestedCommand
}
